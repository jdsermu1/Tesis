{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Tesis\n",
    "En este cuaderno se realiza la creacion de arquitecturas para modelos de la tesis de deteccion de retinopatia diabetica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones\n",
    "A continuacion se realizan las importaciones de librreries y frameworks necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os, gc, glob, math, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPool2D, Dense, LeakyReLU, Reshape, InputSpec, Dropout, Flatten, LeakyReLU, Softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, TerminateOnNaN, TensorBoard\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3080, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices:\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=9000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "#tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "# tf.config.gpu.set_per_process_memory_fraction(0.87)\n",
    "# tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de carpetas\n",
    "En esta celda se definen las carpetas de donde se busca cargar las iamgenes en cada batch, se opto por esta estretagia debido a que el numero de imagenes no suele caber en memoria. Es importante anotar que dependiendo de la estrategia a probar se debe modificar una linea de codigo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "db_folder_kaggle = os.path.join(os.getcwd(), '../DR Databases/Kaggle')\n",
    "# Dependiendo de la estrategia a probar cambiar nombre carpeta\n",
    "strategy = \"strategy 1\"\n",
    "images_folder = os.path.join(db_folder_kaggle, strategy)\n",
    "all_images_folder = os.path.join(db_folder_kaggle, \"preprocessed images\")\n",
    "# ------------------------------------------------------------\n",
    "labels_path = os.path.join(db_folder_kaggle, \"labels.csv\")\n",
    "train_images_folder = os.path.join(images_folder, \"train\")\n",
    "validation_images_folder = os.path.join(images_folder, \"validation\")\n",
    "test_images_folder = os.path.join(images_folder, \"test\")\n",
    "log_dir = os.path.join(db_folder_kaggle, \"logs\")\n",
    "current_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas y Clases de la Medida\n",
    "A continuacion se presentan las clases personalizadas para la asistencia de la creacion del modelo. La principal siendo la capa FractionalPooling con la cual se implementa la estrategia de fractional max pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalPooling2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_ratio = None, pseudo_random = False, overlap = False, name ='FractionPooling2D', **kwargs):\n",
    "        self.pool_ratio = pool_ratio\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "        self.pseudo_random = pseudo_random\n",
    "        self.overlap = overlap\n",
    "        super(FractionalPooling2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, input):\n",
    "        batch_tensor,_,_ = tf.nn.fractional_max_pool(input, pooling_ratio = self.pool_ratio, \n",
    "                                                     pseudo_random = self.pseudo_random, \n",
    "                                                     overlapping = self.overlap)\n",
    "        return batch_tensor\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if(K.image_dim_ordering() == 'channels_last' or K.image_dim_ordering() == 'tf'):\n",
    "            if(input_shape[0] != None):\n",
    "                batch_size = int(input_shape[0]/self.pool_ratio[0])\n",
    "            else:\n",
    "                batch_size = input_shape[0]\n",
    "            width = int(input_shape[1]/self.pool_ratio[1])\n",
    "            height = int(input_shape[2]/self.pool_ratio[2])\n",
    "            channels = int(input_shape[3]/self.pool_ratio[3])\n",
    "            return(batch_size, width, height, channels)\n",
    "\n",
    "        elif(K.image_dim_ordering() == 'channels_first' or K.image_dim_ordering() == 'th'):\n",
    "            if(input_shape[0] != None):\n",
    "                batch_size = int(input_shape[0]/self.pool_ratio[0])\n",
    "            else:\n",
    "                batch_size = input_shape[0]\n",
    "            channels = int(input_shape[1]/self.pool_ratio[1])\n",
    "            width = int(input_shape[2]/self.pool_ratio[2])\n",
    "            height = int(input_shape[3]/self.pool_ratio[3])\n",
    "            return(batch_size, channels, width, height)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pooling_ratio': self.pool_ratio, 'pseudo_random': self.pseudo_random, 'overlap': self.overlap, \n",
    "                  'name':self.name}\n",
    "        base_config = super(FractionalPooling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = InputSpec(shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion funcion modelos\n",
    "En el siguiente bloque se definen los diferentes modelos encontrados en la literatura. A partir de ellos se busca implementar la mejora a la clasificacion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_A(kernel_max_norm=0.1, input_shape=(540, 540, 3)):\n",
    "    initializer =  None #HeUniform()\n",
    "    constraint = None # MaxNorm(max_value=kernel_max_norm, axis=[0,1,2])\n",
    "    constraint_bias = None\n",
    "    fmp_type = 'float32'\n",
    "    fmp_overlap = True\n",
    "    fmp_pesudo_random = True\n",
    "    conv_padding = 'same'\n",
    "    pool_ratio = (1, 1.8, 1.8, 1)\n",
    "    leakyr_alpha = 0.333\n",
    "    \n",
    "    m = Sequential()\n",
    "    \n",
    "    m.add(Conv2D(32,(5, 5), input_shape=input_shape, padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(64,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(96,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(128,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "\n",
    "    m.add(Conv2D(160,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(192,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(224,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(256,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(Dropout(32.0/352))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(288,(2, 2), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(Dropout(32.0/384))\n",
    "    m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    \n",
    "    m.add(Conv2D(320,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(Dropout(64.0/416))\n",
    "    \n",
    "    m.add(Conv2D(356,(1, 1), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    l = Dropout(64.0/448)\n",
    "    m.add(l)\n",
    "    \n",
    "    m.add(Conv2D(5,l.output_shape[1:3], kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(Softmax(dtype='float32'))\n",
    "    m.add(Flatten())\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "def model_B(kernel_max_norm=0.1):\n",
    "    initializer =  None # HeUniform()\n",
    "    constraint = None # MaxNorm(max_value=kernel_max_norm, axis=[0,1,2])\n",
    "    constraint_bias = None\n",
    "\n",
    "    m = Sequential()\n",
    "    m.add(Conv2D(32,(5, 5), input_shape=(540, 540, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True))\n",
    "    \n",
    "    m.add(Conv2D(64,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(96,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(128,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(160,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(192,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(224,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(256,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(288,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(320,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    \n",
    "    m.add(Conv2D(352,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.6, 1.6, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    m.add(Dropout(32.0/352))\n",
    "    \n",
    "    m.add(Conv2D(384,(3, 3), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(FractionalPooling2D(pool_ratio=(1, 1.5, 1.5, 1),pseudo_random = True,overlap=True, dtype='float32'))\n",
    "    m.add(Dropout(32.0/384))\n",
    "    \n",
    "    \n",
    "    m.add(Conv2D(416,(2, 2), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    m.add(Dropout(64.0/416))\n",
    "    \n",
    "    m.add(Conv2D(448,(1, 1), padding='same', kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=0.333))\n",
    "    l = Dropout(64.0/448)\n",
    "    m.add(l)\n",
    "    \n",
    "    m.add(Conv2D(5,l.output_shape[1:3], kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(Softmax(dtype='float32'))\n",
    "    m.add(Flatten())\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "\n",
    "def model_A_modified(kernel_max_norm=0.1, input_shape=(540, 540, 3)):\n",
    "    \n",
    "    initializer =  None #HeUniform()\n",
    "    constraint = None # MaxNorm(max_value=kernel_max_norm, axis=[0,1,2])\n",
    "    constraint_bias = None\n",
    "    fmp_type = 'float32'\n",
    "    fmp_overlap = True\n",
    "    fmp_pesudo_random = True\n",
    "    conv_padding = 'same'\n",
    "    pool_ratio = (1, 1.8, 1.8, 1)\n",
    "    leakyr_alpha = 0.333\n",
    "    \n",
    "    m = Sequential()\n",
    "    \n",
    "    m.add(Conv2D(32,(5, 5), input_shape=input_shape, padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(64,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(96,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(128,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "\n",
    "    m.add(Conv2D(160,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(192,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(224,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "    m.add(Conv2D(256,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    m.add(Dropout(32.0/352))\n",
    "#     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "    m.add(MaxPool2D())\n",
    "    \n",
    "#     m.add(Conv2D(288,(2, 2), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "#     m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(Dropout(32.0/384))\n",
    "# #     m.add(FractionalPooling2D(pool_ratio=pool_ratio,pseudo_random = fmp_pesudo_random,overlap=fmp_overlap, dtype=fmp_type))\n",
    "#     m.add(MaxPool2D())\n",
    "    \n",
    "#     m.add(Conv2D(320,(3, 3), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "#     m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "#     m.add(Dropout(64.0/416))\n",
    "    \n",
    "    m.add(Conv2D(356,(1, 1), padding=conv_padding, kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(LeakyReLU(alpha=leakyr_alpha))\n",
    "    l = Dropout(64.0/448)\n",
    "    m.add(l)\n",
    "    \n",
    "    m.add(Conv2D(5,l.output_shape[1:3], kernel_initializer=initializer, kernel_constraint=constraint, bias_constraint=constraint_bias))\n",
    "    m.add(Softmax(dtype='float32'))\n",
    "    m.add(Flatten())\n",
    "    m.summary()\n",
    "    return m\n",
    "   \n",
    "\n",
    "def model_D(input_shape):\n",
    "    m = tf.keras.applications.VGG16(include_top=False, weights='imagenet',input_shape=input_shape, pooling=\"max\")\n",
    "    m.get_layer(\"block5_conv3\").trainable = False\n",
    "    last_layer = m.get_layer(\"block5_pool\")\n",
    "    x = Flatten()(last_layer.output)\n",
    "    x = Dense(4096, activation=\"relu\")(x)\n",
    "    x = Dense(4096, activation=\"relu\")(x)\n",
    "    x = Dense(5, activation=\"softmax\")(x)\n",
    "    m = Model(m.input, x)\n",
    "    m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('val_accuracy')>0.8476):\n",
    "            print(\"Reached reported accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "    \n",
    "    def on_test_batch_begin(self, epoch, logs={}):\n",
    "        gc.collect()\n",
    "            \n",
    "def scheduler(epoch, lr):\n",
    "    return lr * math.exp(-0.05)\n",
    "\n",
    "def schedulerD(epoch, lr):\n",
    "    if epoch%5==0:\n",
    "        return math.pow(lr, 15)\n",
    "    else:\n",
    "        return lr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_generator(batch_size, img_dims):\n",
    "    train_generator = ImageDataGenerator(rotation_range=360, shear_range=.2, rescale=1.0/255.0)\n",
    "    train_flow = train_generator.flow_from_directory(train_images_folder, \n",
    "                                                batch_size=batch_size, \n",
    "                                                class_mode=\"categorical\", \n",
    "                                                target_size = img_dims)\n",
    "    validation_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    validation_flow = validation_generator.flow_from_directory(validation_images_folder, \n",
    "                                       batch_size=batch_size, \n",
    "                                       class_mode=\"categorical\", \n",
    "                                       target_size = img_dims)\n",
    "    data_generator = {\"train\": train_flow,\"validation\":validation_flow}\n",
    "    return data_generator\n",
    "\n",
    "rescaling = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.0)\n",
    "rotation = tf.keras.layers.experimental.preprocessing.RandomRotation((0, 1),fill_mode='nearest')\n",
    "flip = tf.keras.layers.experimental.preprocessing.RandomFlip()\n",
    "\n",
    "def load_data_using_tfdata(folders, batch_size, img_dims, caches, parallels):\n",
    "    \n",
    "    def parse_image(file_path):\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        class_names = np.array(os.listdir(images_folder + '/train'))\n",
    "        label = parts[-2] == class_names\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [img_dims[0], img_dims[1]])\n",
    "        img = rescaling(img)\n",
    "        return img, label\n",
    "    \n",
    "    def data_augmentation(x, y):\n",
    "        x = rotation(x)\n",
    "        x = flip(x)\n",
    "        return x, y\n",
    "        \n",
    "    def prepare_for_training(ds, cache=f'./{strategy}.cache', shuffle_buffer_size=200, folder_name=\"train\", parallel = tf.data.AUTOTUNE):\n",
    "        if cache:\n",
    "            if isinstance(cache, str):\n",
    "                ds = ds.cache(cache)\n",
    "            else:\n",
    "                ds = ds.cache()\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size).repeat().batch(batch_size)\n",
    "        if folder_name== \"train\":\n",
    "            ds = ds.map(data_augmentation, num_parallel_calls=parallel)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "\n",
    "    data_generator = {}\n",
    "    for i, x in enumerate(folders):\n",
    "        dir_extend = images_folder + '/' + x\n",
    "        list_ds = tf.data.Dataset.list_files(glob.glob(os.path.join(dir_extend, \"**\", '*.jpeg')))\n",
    "        labeled_ds = list_ds.map(parse_image, num_parallel_calls=parallels[i])\n",
    "        data_generator[x] = prepare_for_training(labeled_ds, cache=caches[i], folder_name=x, parallel=parallels[i])\n",
    "    return data_generator\n",
    "\n",
    "\n",
    "def load_data(validation_size, test_size, batch_size=32, img_dims=(540, 540), \n",
    "                          caches={\"train\": False, \"test\":False, \"validation\": False}, parallel=5):\n",
    "\n",
    "    assert(0<validation_size<1 and 0<test_size<1 and validation_size+test_size<1)\n",
    "    \n",
    "    def parse_image(img_name, y):\n",
    "        file_path = tf.strings.join([all_images_folder+os.path.sep,img_name, \".jpeg\"])\n",
    "        label = tf.one_hot(y, 5)\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [img_dims[0], img_dims[1]])\n",
    "        img = rescaling(img)\n",
    "        return img, label\n",
    "    \n",
    "    def data_augmentation(x, y):\n",
    "        x = rotation(x)\n",
    "        x = flip(x)\n",
    "        return x, y\n",
    "        \n",
    "    def prepare_for_training(ds,folder_name, shuffle_buffer_size=200):\n",
    "        if caches[folder_name]:\n",
    "            if isinstance(caches[folder_name], str):\n",
    "                ds = ds.cache(caches[folder_name])\n",
    "            else:\n",
    "                ds = ds.cache()\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.batch(batch_size)\n",
    "        if folder_name== \"train\":\n",
    "            ds = ds.map(data_augmentation, num_parallel_calls=parallel)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    \n",
    "    data_df = pd.read_csv(labels_path)\n",
    "    temp_df, data_test_df = train_test_split(data_df, test_size=test_size, stratify=data_df[\"level\"])\n",
    "    data_train_df, data_validation_df = train_test_split(temp_df, test_size=validation_size/(1-test_size), stratify=temp_df[\"level\"])\n",
    "    data_dfs = {\"train\": data_train_df, \"test\": data_test_df, \"validation\": data_validation_df}\n",
    "    data_dfs_len = {\"train\": len(data_train_df), \"test\": len(data_test_df), \"validation\": len(data_validation_df)}\n",
    "    print(data_df.groupby(\"level\").size(), data_test_df.groupby(\"level\").size(), data_train_df.groupby(\"level\").size(), \n",
    "         data_validation_df.groupby(\"level\").size())\n",
    "    \n",
    "    data_generator = {}\n",
    "    \n",
    "    for set_name, set_df in data_dfs.items():\n",
    "        set_ds = tf.data.Dataset.from_tensor_slices((tf.cast(set_df[\"image\"].values, tf.string),\n",
    "                                                     tf.cast(set_df['level'].values, tf.int32)))\n",
    "        labeled_ds = set_ds.map(parse_image, num_parallel_calls=parallel)\n",
    "        data_generator[set_name] = prepare_for_training(labeled_ds, folder_name=set_name)\n",
    "    \n",
    "    return data_generator\n",
    "\n",
    "\n",
    "def load_data_balanced_test_val(validation_size, test_size, train_size=None, balance = False, batch_size=32, img_dims=(540, 540), \n",
    "                          caches={\"train\": False, \"test\":False, \"validation\": False}, parallel=5):\n",
    "    \n",
    "    def parse_image(img_name, y):\n",
    "        file_path = tf.strings.join([all_images_folder+os.path.sep,img_name, \".jpeg\"])\n",
    "        label = tf.one_hot(y, 5)\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [img_dims[0], img_dims[1]], method='nearest')\n",
    "        img = rescaling(img)\n",
    "        return img, label\n",
    "    \n",
    "    def data_augmentation(x, y):\n",
    "        x = rotation(x)\n",
    "        x = flip(x)\n",
    "        return x, y\n",
    "        \n",
    "    def prepare_for_training(ds,folder_name, shuffle_buffer_size=200):\n",
    "        if caches[folder_name]:\n",
    "            if isinstance(caches[folder_name], str):\n",
    "                ds = ds.cache(caches[folder_name])\n",
    "            else:\n",
    "                ds = ds.cache()\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.batch(batch_size)\n",
    "        if folder_name== \"train\":\n",
    "            ds = ds.map(data_augmentation, num_parallel_calls=parallel)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    data_df = pd.read_csv(labels_path)\n",
    "    data_train_df, data_test_df, data_validation_df = None, None, None\n",
    "    \n",
    "    for label in range(5):\n",
    "        data_label_df = data_df[data_df[\"level\"]==label]\n",
    "        temp_df, data_label_test_df = train_test_split(data_label_df, test_size=test_size)\n",
    "        data_label_train_df, data_label_validation_df = train_test_split(temp_df, test_size=validation_size)\n",
    "        if train_size and len(data_label_train_df)>train_size:\n",
    "            data_label_train_df, _ = train_test_split(data_label_train_df, train_size=train_size)\n",
    "        \n",
    "        if data_test_df is None: \n",
    "            data_test_df = data_label_test_df \n",
    "        else: \n",
    "            data_test_df = pd.concat([data_test_df, data_label_test_df])\n",
    "        if data_validation_df is None: \n",
    "            data_validation_df = data_label_validation_df \n",
    "        else: \n",
    "            data_validation_df = pd.concat([data_validation_df, data_label_validation_df])\n",
    "        if data_train_df is None: \n",
    "            data_train_df = data_label_train_df \n",
    "        else: \n",
    "            data_train_df = pd.concat([data_train_df, data_label_train_df])\n",
    "            \n",
    "            \n",
    "    if balance:\n",
    "        ros = RandomOverSampler()\n",
    "        data_train_df, _ = ros.fit_resample(data_train_df, data_train_df[\"level\"]) \n",
    "            \n",
    "        \n",
    "    data_dfs = {\"train\": data_train_df, \"test\": data_test_df, \"validation\": data_validation_df}\n",
    "    data_dfs_len = {\"train\": len(data_train_df), \"test\": len(data_test_df), \"validation\": len(data_validation_df)}\n",
    "    \n",
    "    data_generator = {}\n",
    "    \n",
    "    for set_name, set_df in data_dfs.items():\n",
    "        set_ds = tf.data.Dataset.from_tensor_slices((tf.cast(set_df[\"image\"].values, tf.string),\n",
    "                                                     tf.cast(set_df['level'].values, tf.int32)))\n",
    "        labeled_ds = set_ds.map(parse_image, num_parallel_calls=parallel)\n",
    "        data_generator[set_name] = prepare_for_training(labeled_ds, folder_name=set_name)\n",
    "    \n",
    "    return data_generator, data_dfs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions {'train': 125000, 'test': 500, 'validation': 1560}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 131,921,221\n",
      "Non-trainable params: 2,359,808\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2083/2083 [==============================] - 629s 294ms/step - loss: 0.1541 - accuracy: 0.9731 - precision: 0.9787 - recall: 0.9688 - val_loss: 14.6747 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 2/30\n",
      "2083/2083 [==============================] - 591s 284ms/step - loss: 0.2980 - accuracy: 0.9581 - precision: 0.9619 - recall: 0.9552 - val_loss: 12.9829 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 3/30\n",
      "2083/2083 [==============================] - 617s 296ms/step - loss: 0.4227 - accuracy: 0.9351 - precision: 0.9382 - recall: 0.9238 - val_loss: 9.3187 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 4/30\n",
      "2083/2083 [==============================] - 599s 287ms/step - loss: 0.6439 - accuracy: 0.8691 - precision: 0.9190 - recall: 0.8247 - val_loss: 12.3683 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 5/30\n",
      "2083/2083 [==============================] - 614s 295ms/step - loss: 0.4871 - accuracy: 0.9002 - precision: 0.9149 - recall: 0.8698 - val_loss: 8.1396 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 6/30\n",
      "2083/2083 [==============================] - 619s 297ms/step - loss: 0.4782 - accuracy: 0.8940 - precision: 0.9048 - recall: 0.8792 - val_loss: 10.2930 - val_accuracy: 0.2000 - val_precision: 0.2000 - val_recall: 0.2000\n",
      "Epoch 7/30\n",
      "2083/2083 [==============================] - 598s 287ms/step - loss: 0.8607 - accuracy: 0.7752 - precision: 0.8886 - recall: 0.6209 - val_loss: 1.7571 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/30\n",
      "2083/2083 [==============================] - 577s 277ms/step - loss: 1.8503 - accuracy: 0.0759 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6648 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/30\n",
      "2083/2083 [==============================] - 576s 277ms/step - loss: 1.7117 - accuracy: 0.0488 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6425 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/30\n",
      "2083/2083 [==============================] - 577s 277ms/step - loss: 1.6535 - accuracy: 0.0828 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6250 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/30\n",
      "2083/2083 [==============================] - 576s 277ms/step - loss: 1.6296 - accuracy: 0.1342 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6251 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/30\n",
      "2083/2083 [==============================] - 587s 282ms/step - loss: 1.6332 - accuracy: 0.1307 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.6228 - val_accuracy: 0.2000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1029/2083 [=============>................] - ETA: 4:57 - loss: 1.6514 - accuracy: 0.1177 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "log_dir_training = os.path.join(log_dir, \"log_\" + current_time.strftime(\"%Y-%m-%d-%H:%M:%S\"))\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(MyCallback()) # Al alcanzar acc estimado del paper\n",
    "callbacks.append(EarlyStopping(monitor=\"val_accuracy\", patience=30, restore_best_weights=True)) # En caso de estancarse\n",
    "#callbacks.append(LearningRateScheduler(schedulerD)) # Para prgramar funcion actualizarcion de tasa aprendizaje\n",
    "callbacks.append(TerminateOnNaN()) # Terminar en caso fallas aprendizaje\n",
    "callbacks.append(TensorBoard(log_dir=log_dir_training, histogram_freq=1)) # Utilizar Tensorboard\n",
    "\n",
    "batch_size = 60\n",
    "input_size = (224, 224, 3)\n",
    "# data_generator = load_data_using_tfdata([\"train\", \"validation\"], \n",
    "#                                         batch_size, input_size[:-1], \n",
    "#                                         [False, False], \n",
    "#                                         [5, 5])\n",
    "# f\"{strategy}.dump\"\n",
    "\n",
    "\n",
    "data_generators, data_generators_dimensions = load_data_balanced_test_val(312, 100, train_size=25000, balance=True, batch_size=batch_size, img_dims=input_size[:-1], \n",
    "                                                                          parallel=5)\n",
    "print(\"Dimensions\", data_generators_dimensions)\n",
    "\n",
    "model = model_D(input_size)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\", Precision(), Recall()],\n",
    "              optimizer=Adam(learning_rate=0.00001))#, momentum=0.999))\n",
    "\n",
    "\n",
    "history = model.fit(data_generators['train'], \n",
    "                    validation_data=data_generators['validation'],\n",
    "                    steps_per_epoch=data_generators_dimensions['train']//batch_size, \n",
    "                    validation_steps=data_generators_dimensions['validation']//batch_size, \n",
    "                    epochs=30, callbacks=callbacks)#,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weight = {0:0.2668 ,1: 2.9911 ,2:1.3601, 3:10.3450, 4:11.5366}\n",
    "# Strategy 1\n",
    "# num_images_train = 80847\n",
    "# num_images_validate = 500\n",
    "# Strategy 2\n",
    "# num_images_train = 7510\n",
    "# num_images_validate = 500\n",
    "# history = model.fit(data_generator['train'], \n",
    "#                     validation_data=data_generator['validation'],\n",
    "#                     steps_per_epoch=num_images_train//batch_size, \n",
    "#                     validation_steps=num_images_validate//batch_size, \n",
    "#                     epochs=30, callbacks=callbacks)#, \n",
    "                    #class_weight=class_weight)\n",
    "# history_df = pd.DataFrame(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
